{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'py_files')\n",
    "from model_recurrent import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and pad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "Loading 100 images\n",
      "Padding images using pad of:  47\n",
      "(100, 494, 494, 3)\n",
      "(100, 494, 494)\n"
     ]
    }
   ],
   "source": [
    "# Load a set of image\n",
    "root_dir = \"../Data/training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = len(files) \n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs_original = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "gt_imgs_not_padded = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "\n",
    "print('Padding images using pad of: ', pad_rotate_size)\n",
    "imgs = padding_imgs(np.array(imgs),pad_rotate_size)\n",
    "gt_imgs = padding_GT(np.array(gt_imgs),pad_rotate_size)\n",
    "print(imgs.shape)\n",
    "print(gt_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split in validation + train: TODO, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:  (100, 494, 494, 3)\n",
      "Batch_size: 250 \n",
      "Steps per epoch: 125 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "py_files\\model_recurrent.py:175: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  callbacks=[lr_callback, stop_callback])\n",
      "py_files\\model_recurrent.py:175: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=125, verbose=1, callbacks=[<keras.ca..., epochs=45)`\n",
      "  callbacks=[lr_callback, stop_callback])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "125/125 [==============================] - 563s 5s/step - loss: 0.5617 - acc: 0.7472 - f1_score: 0.7472\n",
      "Epoch 2/45\n",
      "125/125 [==============================] - 562s 4s/step - loss: 0.5265 - acc: 0.7442 - f1_score: 0.7442\n",
      "Epoch 3/45\n",
      "125/125 [==============================] - 575s 5s/step - loss: 0.4992 - acc: 0.7606 - f1_score: 0.7606\n",
      "Epoch 4/45\n",
      "125/125 [==============================] - 571s 5s/step - loss: 0.4601 - acc: 0.7801 - f1_score: 0.7801\n",
      "Epoch 5/45\n",
      "125/125 [==============================] - 550s 4s/step - loss: 0.4313 - acc: 0.7984 - f1_score: 0.7984\n",
      "Epoch 6/45\n",
      "125/125 [==============================] - 545s 4s/step - loss: 0.3771 - acc: 0.8322 - f1_score: 0.8322\n",
      "Epoch 7/45\n",
      "125/125 [==============================] - 548s 4s/step - loss: 0.3629 - acc: 0.8399 - f1_score: 0.8399\n",
      "Epoch 8/45\n",
      " 42/125 [=========>....................] - ETA: 6:21 - loss: 0.3425 - acc: 0.8514 - f1_score: 0.8514"
     ]
    }
   ],
   "source": [
    "X_train = imgs\n",
    "Y_train = gt_imgs\n",
    "model = train(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# if we want to go on with the training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = imgs\n",
    "# Y_train = gt_imgs\n",
    "# model, lr_callback, stop_callback = create_model()\n",
    "# model.load_weights('NicolaWeights0612')\n",
    "\n",
    "# np.random.seed(3) # We don't want to take the same data\n",
    "    \n",
    "# try:\n",
    "#     model.fit_generator(generate_minibatch(X_train,Y_train),\n",
    "#                             steps_per_epoch=steps_per_epoch,\n",
    "#                             nb_epoch=3,\n",
    "#                             verbose=1,\n",
    "#                             callbacks=[lr_callback, stop_callback])\n",
    "# except KeyboardInterrupt:\n",
    "#     print('\\n\\nKeyboard interruption!\\n\\n')\n",
    "#     pass\n",
    "# model.save_weights('NicolaWeights2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if on the train set we have good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IDX = 1\n",
    "X = imgs_to_windows(imgs_original,400,patch_size,input_size)\n",
    "patches_idx = X[625*IDX:625*(IDX+1)]\n",
    "\n",
    "Z_idx = model.predict(patches_idx)\n",
    "labels_idx = (Z_idx[:,0] < Z_idx[:,1]) * 1 \n",
    "predicted_image = label_to_img(400,400,16,16,labels_idx)\n",
    "temp = concatenate_images(imgs_original[IDX],predicted_image)\n",
    "plt.imshow(concatenate_images(temp, gt_imgs_not_padded[IDX]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the weights without training everything again\n",
    "model,_,_ = create_model()\n",
    "model.load_weights('model_2b_Weights')\n",
    "\n",
    "N_valid = 3\n",
    "N = 100\n",
    "X_valid = imgs_original[N-N_valid:]\n",
    "X_valid = padding_imgs(np.array(X_valid),pad_size)\n",
    "\n",
    "#create the input for the model\n",
    "val_inputs = imgs_to_windows(X_valid,400,patch_size,input_size)\n",
    "print(val_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the true value of the labels\n",
    "Y_valid = gt_imgs_not_padded[N-N_valid:]\n",
    "val_gt_patches = [img_crop(Y_valid[i], patch_size, patch_size) for i in range(N_valid)]\n",
    "val_gt_patches =  np.asarray([val_gt_patches[i][j] for i in range(len(val_gt_patches)) for j in range(len(val_gt_patches[i]))])\n",
    "val_true_labels = np.asarray([value_to_class(np.mean(val_gt_patches[i])) for i in range(len(val_gt_patches))])\n",
    "print(val_true_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "val_prediction = model.predict(val_inputs)\n",
    "print(val_prediction[0:3])\n",
    "# compute the metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "val_predicted_labels = ((val_prediction[:,0] < val_prediction[:,1]) * 1 ).flatten()\n",
    "val_accuracy = np.sum(np.abs(val_predicted_labels - val_true_labels))/val_true_labels.shape[0]\n",
    "print('Accuracy on validation set is: ', val_accuracy)\n",
    "print('F1 score on validation set is:', f1_score(val_true_labels,val_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images and pad them\n",
    "test_images = np.asarray(pick_test_images())\n",
    "\n",
    "test_images = padding_imgs(np.array(test_images),pad_size)\n",
    "\n",
    "test_images_not_padded = np.asarray(pick_test_images())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the input for the prediction\n",
    "test_inputs = imgs_to_windows(test_images,608,patch_size,input_size)\n",
    "print(test_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recover the model\n",
    "model, _,_ = create_model()\n",
    "model.load_weights('model_2b_Weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "test_prediction = model.predict(test_inputs)\n",
    "print(test_prediction[0:3])\n",
    "test_predicted_labels = ((test_prediction[:,0] < test_prediction[:,1]) * 1 ).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_predicted_labels.reshape(50,-1)\n",
    "print('Every row contains the labels of one image')\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check the prediction on the image IDX\n",
    "IDX = 18\n",
    "im = label_to_img(608, 608, 16, 16, test_labels[IDX])\n",
    "plt.figure(figsize=[20,30])\n",
    "new_img = make_img_overlay(test_images_not_padded[IDX], im)\n",
    "plt.imshow(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_masks_to_submission(SubmissionName, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Suppose that we create some patches\n",
    "predicted_patches = test_predicted_labels\n",
    "print('Predicted patches of size: ', predicted_patches.shape)\n",
    "\n",
    "\n",
    "# Choose a name for the objects that we are going to save\n",
    "name = 'prediction1212.pkl'\n",
    "\n",
    "# Saving the objects:\n",
    "with open(name, 'wb') as f:\n",
    "    pickle.dump(predicted_patches, f)\n",
    "print('Saved in ', name)\n",
    "\n",
    "# Getting back the objects:\n",
    "with open(name, 'rb') as f: \n",
    "    recovered_patches = pickle.load(f)\n",
    "print('Recovered!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

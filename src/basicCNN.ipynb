{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from somefunctions import *\n",
    "augmented = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration parameters\n",
    "Easier if we put them all togheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1e-6  #regularization term\n",
    "pool_size = (2, 2)\n",
    "train_shape = 400 #size of the training images\n",
    "patch_size = 16\n",
    "input_size = 80 # for now is the patch size\n",
    "pad_size = int(input_size/2 - patch_size/2)\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "nb_epoch = 2 #very small, only preliminary tests\n",
    "steps_per_epoch = 50 #the number of training samples is huge, arbitrary value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate the minibatch\n",
    "This function will be called during the training. <br>\n",
    "Here we can also do some feature augmentation, for example. But maybe is more efficient to flip all the images before. <br>\n",
    "TODO: take a random patch of dimension (input_size x input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatch(X,Y):\n",
    "    \"\"\"\n",
    "    Generate a minibatch\n",
    "    \"\"\"\n",
    "    while 1:\n",
    "        # Generate one minibatch\n",
    "        X_batch = np.empty((batch_size, input_size, input_size, 3))\n",
    "        Y_batch = np.empty(batch_size)\n",
    "        low=input_size//2\n",
    "        high = (train_shape + 2*pad_size - input_size//2)\n",
    "        for i in range(batch_size):\n",
    "            # Select a random image\n",
    "            idx = np.random.choice(X.shape[0])\n",
    "            \n",
    "            x_coord = np.random.randint(low=low, high = high ) \n",
    "            y_coord = np.random.randint(low=low, high = high )\n",
    "      \n",
    "            X_batch[i] = X[idx,x_coord - input_size//2:x_coord + input_size//2,y_coord - input_size//2:y_coord + input_size//2]\n",
    "            \n",
    "            gt_batch = Y[idx,x_coord - patch_size//2:x_coord + patch_size//2,y_coord - patch_size//2:y_coord + patch_size//2]            \n",
    "            Y_batch[i] = patch_to_label(gt_batch)\n",
    "            \n",
    "            #TODO: TRY TO CHANGE CONTRAST AND BRIGHTNESS\n",
    "            \n",
    "            \n",
    "        yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "Generate the model and train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(64, kernel_size=(5, 5), \n",
    "                            input_shape = ( input_size, input_size, 3), \n",
    "                            activation = 'relu',\n",
    "                            padding = 'SAME'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 128, activation = 'relu'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    opt = Adam(lr=learning_rate) # Adam optimizer with default initial learning rate\n",
    " \n",
    "\n",
    "    # This callback reduces the learning rate when the training accuracy does not improve any more\n",
    "    lr_callback = ReduceLROnPlateau(monitor='acc', factor=0.5, patience=5,\n",
    "                                    verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    \n",
    "    # Stops the training process upon convergence\n",
    "    stop_callback = EarlyStopping(monitor='acc', min_delta=0.0001, patience=11, verbose=1, mode='auto')\n",
    "    \n",
    "    model.compile(loss=binary_crossentropy,\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model, stop_callback, lr_callback\n",
    "\n",
    "    \n",
    "\n",
    "def train(X, Y):    \n",
    "    '''\n",
    "    Generate an instance of the model an train the model on X, Y\n",
    "    '''\n",
    "    print('Training set shape: ', X.shape) \n",
    "    print(f'Batch_size: {batch_size} \\nSteps per epoch: {steps_per_epoch} \\n')\n",
    "    \n",
    "    \n",
    "    model, stop_callback, lr_callback = create_model()\n",
    "    \n",
    "    np.random.seed(20122018) # Reproducibility + remember the deadline is the 20.12.2018\n",
    "    \n",
    "    try:\n",
    "        model.fit_generator(generate_minibatch(X,Y),\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            verbose=1,\n",
    "                            callbacks=[lr_callback, stop_callback])\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nKeyboard interruption!')\n",
    "        pass\n",
    "    \n",
    "    print('Training completed')\n",
    "    \n",
    "    model.save_weights('pesi')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of image\n",
    "root_dir = \"../Data/training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = len(files) \n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs_original = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augmenting\n",
    "if not augmented:\n",
    "    #flip\n",
    "    for i in range(n):\n",
    "        imgs.append(np.fliplr(imgs[i]))\n",
    "        imgs.append(np.flipud(imgs[i]))\n",
    "        gt_imgs.append(np.fliplr(gt_imgs[i]))\n",
    "        gt_imgs.append(np.flipud(gt_imgs[i]))\n",
    "    \n",
    "    #rotate\n",
    "    for i in range(3*n):\n",
    "        imgs.append(np.rot90(imgs[i],1))\n",
    "        imgs.append(np.rot90(imgs[i],2))\n",
    "        imgs.append(np.rot90(imgs[i],3))\n",
    "        gt_imgs.append(np.rot90(gt_imgs[i],1))\n",
    "        gt_imgs.append(np.rot90(gt_imgs[i],2))\n",
    "        gt_imgs.append(np.rot90(gt_imgs[i],3))\n",
    "        \n",
    "    augmented = True\n",
    "        \n",
    "\n",
    "imgs = padding_imgs(np.array(imgs),pad_size)\n",
    "gt_imgs = padding_GT(np.array(gt_imgs),pad_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split in validation + train\n",
    "\n",
    "(VERY BASIC!) In this way we can check directly what happens in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = imgs.shape[0]\n",
    "ratio = 0.8\n",
    "N_train = int(N*ratio)\n",
    "N_valid = int(N - N*ratio)\n",
    "X_train = imgs[:N_train]\n",
    "X_valid = imgs[N-N_valid:]\n",
    "Y_train = gt_imgs[:N_train]\n",
    "Y_valid = gt_imgs[N-N_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_train.shape[0] + X_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if on the train set we have good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_to_windows(imgs, img_size, patch_size, window_size):\n",
    "    ''' Takes padded images and outputs an array with the windows'''\n",
    "    windows = []\n",
    "    for idx in range(imgs.shape[0]):\n",
    "        im = imgs[idx]\n",
    "        for i in range(img_size//patch_size):\n",
    "            for j in range(img_size//patch_size):\n",
    "                temp = im[j*patch_size:window_size + j*patch_size,\n",
    "                                  i*patch_size:window_size + i*patch_size]\n",
    "                windows.append(temp)\n",
    "    return np.asarray(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    im = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            im[j:j+w, i:i+h] = labels[idx]\n",
    "            idx = idx + 1\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 1\n",
    "X = imgs_to_windows(imgs[1:3],400,patch_size,input_size)\n",
    "patches_idx = X[625*IDX:625*(IDX+1)]\n",
    "\n",
    "patches_idx.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_idx = model.predict(patches_idx)\n",
    "labels_idx = (Z_idx > 0.5) *1 \n",
    "predicted_image = label_to_img(400,400,16,16,labels_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(concatenate_images(imgs_original[IDX],predicted_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = np.asarray(pick_test_images())\n",
    "test_images = padding_imgs(np.array(test_images),pad_size)\n",
    "\n",
    "\n",
    "n_test = test_images.shape[0]\n",
    "\n",
    "test_inputs = imgs_to_windows(test_images,608,16,80)\n",
    "print(test_inputs.shape)\n",
    "\n",
    "model, _,_ = create_model()\n",
    "model.load_weights('pesi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1D = (Z < 0.5 ) * 1\n",
    "labels1D.shape\n",
    "labels = labels1D.reshape(50,-1)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 9\n",
    "im = label_to_img(608, 608, 16, 16, labels[IDX])\n",
    "plt.imshow(concatenate_images(test_images[IDX], im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_masks_to_submission('dummsdsy_submission.csv', labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training example of the final model\n",
    "\n",
    "In this notebook, we illustrate the procedure applied to select the <b>final architecture</b>. \n",
    "\n",
    "* This notebook does <b>NOT</b> reproduce the score on CrowdAI.\n",
    "This is because here we only want to show the procedure to select the best network architecture that we used. In order to select the best architecture we have split the images of the data in <b>80/20% train/validation sets</b> and train it on the corresponding set. This notebook shows this procedure applied to our pre-selected best architecture. However, to obtain the final weights that give our CrowdAI score, we have trained the same architecture on the whole dataset. Moreover, we have also applied a post-processing to the obtained prediction.\n",
    "\n",
    "* This model has <b>2 neurons</b> in the final layer.\n",
    "The only difference between the training of a model with only one neuron at the final layer is that here we cannot rely on our implementation of the F1-score when we choose the `metrics` in `model.compile`. <br>\n",
    "Hence, to perform a validation, we need to split manually the data at the very beginning and compute the F1-score in the local validation set only once the training has been finished.\n",
    "\n",
    "* The <b>final weights</b> can be obtained by setting the validation ratio to 1 in the fourth cell of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../src/')\n",
    "sys.path.insert(0,'../src/models/')\n",
    "from training_and_prediction import *\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config:  choose the model and the root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from best_model import MODEL_CLASS\n",
    "ROOT_DIR = '../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the model class and print some infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main model attributes:\n",
      "\tpatch_size =  16\n",
      "\tinput_size =  64\n",
      "\tpad_size =  24\n",
      "\tpad_rotate_size =  47\n",
      "\tfinal_layer_units =  2\n",
      "\tpool_size =  (2, 2)\n",
      "\n",
      "Training parameters:\n",
      "\tlearning_rate =  0.001\n",
      "\tepochs =  40\n",
      "\tbatch_size =  128\n",
      "\tsteps_per_epoch =  250\n",
      "\n",
      "Other attributes:\n",
      "\tNameWeights =  model_best_Weights\n",
      "\tSubmissionName =  model_best_Submission.csv\n",
      "\tPredictionName =  model_best_prediction\n",
      "\n",
      "VGG-like model with 2 neurons in the final layer.\n",
      "Keras model summary\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 9,350,050\n",
      "Trainable params: 9,350,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MODEL = MODEL_CLASS()\n",
    "MODEL.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (80/20 train/validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "Loading 100 groundtruth images\n",
      "Shape of imgs:  (100, 400, 400, 3)\n",
      "Shape of gt_imgs:  (100, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "X, Y = LoadImages(root_dir = ROOT_DIR)\n",
    "ValidationRatio = 0.8\n",
    "idx_tr, idx_val = get_idx_split_data(X.shape[0], ValidationRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding images using pad of:  47\n",
      "Training the model.\n",
      "Training (padded) images shape:  (80, 494, 494, 3)\n",
      "Epochs: 40\n",
      "Batch_size: 128\n",
      "Steps per epoch: 250\n",
      "No validation data.\n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "250/250 [==============================] - 934s 4s/step - loss: 0.5585 - acc: 0.7467\n",
      "Epoch 2/40\n",
      "250/250 [==============================] - 553s 2s/step - loss: 0.5111 - acc: 0.7464\n",
      "Epoch 3/40\n",
      "250/250 [==============================] - 567s 2s/step - loss: 0.4535 - acc: 0.7769\n",
      "Epoch 4/40\n",
      "250/250 [==============================] - 573s 2s/step - loss: 0.3829 - acc: 0.8241\n",
      "Epoch 5/40\n",
      "250/250 [==============================] - 582s 2s/step - loss: 0.3398 - acc: 0.8508\n",
      "Epoch 6/40\n",
      "250/250 [==============================] - 573s 2s/step - loss: 0.3118 - acc: 0.8653\n",
      "Epoch 7/40\n",
      "250/250 [==============================] - 581s 2s/step - loss: 0.2997 - acc: 0.8711\n",
      "Epoch 8/40\n",
      "250/250 [==============================] - 601s 2s/step - loss: 0.2905 - acc: 0.8752\n",
      "Epoch 9/40\n",
      "250/250 [==============================] - 599s 2s/step - loss: 0.2809 - acc: 0.8813\n",
      "Epoch 10/40\n",
      "250/250 [==============================] - 592s 2s/step - loss: 0.2711 - acc: 0.8846\n",
      "Epoch 11/40\n",
      "250/250 [==============================] - 597s 2s/step - loss: 0.2634 - acc: 0.8900\n",
      "Epoch 12/40\n",
      "250/250 [==============================] - 612s 2s/step - loss: 0.2486 - acc: 0.8962\n",
      "Epoch 13/40\n",
      "250/250 [==============================] - 621s 2s/step - loss: 0.2484 - acc: 0.8959\n",
      "Epoch 14/40\n",
      "250/250 [==============================] - 626s 3s/step - loss: 0.2387 - acc: 0.9007\n",
      "Epoch 15/40\n",
      "250/250 [==============================] - 620s 2s/step - loss: 0.2299 - acc: 0.9040\n",
      "Epoch 16/40\n",
      "250/250 [==============================] - 627s 3s/step - loss: 0.2302 - acc: 0.9041\n",
      "Epoch 17/40\n",
      "250/250 [==============================] - 628s 3s/step - loss: 0.2273 - acc: 0.9061\n",
      "Epoch 18/40\n",
      "250/250 [==============================] - 622s 2s/step - loss: 0.2232 - acc: 0.9095\n",
      "Epoch 19/40\n",
      "250/250 [==============================] - 624s 2s/step - loss: 0.2176 - acc: 0.9108\n",
      "Epoch 20/40\n",
      "250/250 [==============================] - 627s 3s/step - loss: 0.2166 - acc: 0.9106\n",
      "Epoch 21/40\n",
      "250/250 [==============================] - 627s 3s/step - loss: 0.2129 - acc: 0.9127\n",
      "Epoch 22/40\n",
      "250/250 [==============================] - 638s 3s/step - loss: 0.2155 - acc: 0.9113\n",
      "Epoch 23/40\n",
      "250/250 [==============================] - 633s 3s/step - loss: 0.2095 - acc: 0.9143\n",
      "Epoch 24/40\n",
      "250/250 [==============================] - 627s 3s/step - loss: 0.2069 - acc: 0.9147\n",
      "Epoch 25/40\n",
      "250/250 [==============================] - 630s 3s/step - loss: 0.2063 - acc: 0.9149\n",
      "Epoch 26/40\n",
      "250/250 [==============================] - 643s 3s/step - loss: 0.2053 - acc: 0.9163\n",
      "Epoch 27/40\n",
      "250/250 [==============================] - 628s 3s/step - loss: 0.1936 - acc: 0.9188\n",
      "Epoch 28/40\n",
      "250/250 [==============================] - 626s 3s/step - loss: 0.2003 - acc: 0.9177\n",
      "Epoch 29/40\n",
      "250/250 [==============================] - 631s 3s/step - loss: 0.1925 - acc: 0.9203\n",
      "Epoch 30/40\n",
      "250/250 [==============================] - 639s 3s/step - loss: 0.1938 - acc: 0.9199\n",
      "Epoch 31/40\n",
      "250/250 [==============================] - 625s 2s/step - loss: 0.1874 - acc: 0.9232\n",
      "Epoch 32/40\n",
      "250/250 [==============================] - 624s 2s/step - loss: 0.1941 - acc: 0.9201\n",
      "Epoch 33/40\n",
      "250/250 [==============================] - 648s 3s/step - loss: 0.1896 - acc: 0.9231\n",
      "Epoch 34/40\n",
      "250/250 [==============================] - 632s 3s/step - loss: 0.1860 - acc: 0.9238\n",
      "Epoch 35/40\n",
      "250/250 [==============================] - 629s 3s/step - loss: 0.1885 - acc: 0.9233\n",
      "Epoch 36/40\n",
      "250/250 [==============================] - 641s 3s/step - loss: 0.1787 - acc: 0.9269\n",
      "Epoch 37/40\n",
      "250/250 [==============================] - 633s 3s/step - loss: 0.1853 - acc: 0.9230\n",
      "Epoch 38/40\n",
      "250/250 [==============================] - 634s 3s/step - loss: 0.1867 - acc: 0.9236\n",
      "Epoch 39/40\n",
      "250/250 [==============================] - 629s 3s/step - loss: 0.1795 - acc: 0.9286\n",
      "Epoch 40/40\n",
      "250/250 [==============================] - 626s 3s/step - loss: 0.1810 - acc: 0.9256\n",
      "Training done, weights saved in: model_best_Weights\n"
     ]
    }
   ],
   "source": [
    "X_tr, Y_tr = X[idx_tr], Y[idx_tr]\n",
    "model = train(X_tr, Y_tr, MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the training, compute F1 score on the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding images using pad of:  24\n",
      "Number of validation images:  20\n",
      "Predicting... \n",
      "Done!\n",
      "F1 score on validation set is: 0.7955652612128339\n"
     ]
    }
   ],
   "source": [
    "X_val, Y_val = X[idx_val], Y[idx_val]\n",
    "ComputeLocalF1Score(X_val,Y_val,MODEL, MODEL.NameWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can continue with the training in this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding images using pad of:  47\n",
      "Loading weights of the model from:  model_best_Weights\n",
      "Restarting training...\n",
      "Training (padded) images shape:  (80, 494, 494, 3)\n",
      "Epochs: 40\n",
      "Batch_size: 128\n",
      "Steps per epoch: 250\n",
      "No validation data.\n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 636s 3s/step - loss: 0.1849 - acc: 0.9245\n",
      "Training done, weights saved in: model_best_Weights_cont\n"
     ]
    }
   ],
   "source": [
    "nameOldWeights = MODEL.NameWeights\n",
    "nameNewWeights = MODEL.NameWeights + '_cont'\n",
    "model = ContinueTrain(X_tr, Y_tr, MODEL, nameOldWeights, nameNewWeights, epochs_cont = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
